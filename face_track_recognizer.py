'''
bot_face_track_recognizer.py

„Ç´„É°„É©Êò†ÂÉè„ÇíÂèñÂæó„Åó„ÄÅÈ°î„ÇíÊ§úÂá∫„Åó„Å¶Ë™çË≠ò„Åô„ÇãÈ°îËøΩË∑°„Ç∑„Çπ„ÉÜ„É†„ÅÆ„Éú„ÉÉ„ÉàÁî®„Çπ„ÇØ„É™„Éó„Éà„Åß„Åô
„Ç´„É°„É©„ÅßÈ°î„ÇíÊ§úÂá∫„Åó„ÄÅÈ°î„ÅÆÁâπÂæ¥„ÇíÊäΩÂá∫„Åó„Å¶ËæûÊõ∏„Å®ÊØîËºÉ„Åó„ÄÅÈ°îË™çË≠ò„ÇíË°å„ÅÑ„Åæ„Åô
„Åæ„Åü„ÄÅÈ°î„ÅÆ‰∏≠ÂøÉ„ÇíÊçâ„Åà„Å¶„Ç´„É°„É©„ÅÆ„Éë„É≥„Å®„ÉÅ„É´„Éà„ÇíÂà∂Âæ°„Åó„ÄÅÈ°î„ÅÆËøΩË∑°„ÇÇË°å„ÅÑ„Åæ„Åô
'''

import cv2
import numpy as np
import time
import subprocess
from pathlib import Path
from collections import Counter
from picamera2 import Picamera2
import json
# from bot_motor_controller import pan_tilt, neopixels_all, neopixels_off

def get_screen_resolution():
    output = subprocess.check_output("xrandr | grep '*' | awk '{print $1}'", shell=True)
    resolution = output.decode('utf-8').strip().split('x')
    width = int(resolution[0])
    height = int(resolution[1])
    return width, height

class Camera():
    def __init__(self):
        # self.cap = cv2.VideoCapture(0)
        # self.cap.set(3, 640)
        # self.cap.set(4, 480)
        self.picam2 = Picamera2()
        # raw„ÇíË®≠ÂÆö„Åô„Çã„Åì„Å®„Åß„Ç´„É°„É©„ÅÆÊúÄÈ´òËß£ÂÉèÂ∫¶„ÇíÂà©Áî®„Åó„ÄÅÁîªËßí„ÇíÂ∫É„Åí„Åæ„Åô„ÄÇÔºàÊåáÂÆö„Åó„Å™„Åë„Çå„Å∞„Éá„Ç∏„Çø„É´„Ç∫„Éº„É†„Åï„Çå„ÅüÁã≠„ÅÑÁîªËßí„Å´„Å™„ÇãÔºâ
        fullReso = self.picam2.camera_properties['PixelArraySize']
        self.picam2.configure(self.picam2.create_preview_configuration(main={"format": "RGB888", "size": (640, 480)}, raw={"size": fullReso}))
        self.picam2.start()

    def get_frame(self):
        #ret, frame = self.cap.read()
        frame = self.picam2.capture_array()
        if frame is not None:
            return frame
        else:
            print("üñ•Ô∏è SYSTEM: „Ç´„É°„É©„Åã„Çâ„ÅÆ„Éï„É¨„Éº„É†ÂèñÂæó„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇ")
            return None

    def release_camera(self):
        self.picam2.stop()
        self.picam2.close()

def face_recognize():
    # „É¢„Éá„É´„ÅÆË™≠„ÅøËæº„Åø
    face_detector_weights = str(Path("dnn_models/face_detection_yunet_2023mar.onnx").resolve())
    #face_detector_weights = str(Path("dnn_models/yunet_s_640_640.onnx").resolve())  # È°îÊ§úÂá∫Áî®„ÅÆweights
    face_detector = cv2.FaceDetectorYN_create(face_detector_weights, "", (0, 0))

    # È°îË≠òÂà•„É¢„Éá„É´„ÇíË™≠„ÅøËæº„ÇÄ
    face_recognizer_weights = str(Path("dnn_models/face_recognizer_fast.onnx").resolve())  # È°îË™çË≠òÁî®„ÅÆweights
    face_recognizer = cv2.FaceRecognizerSF_create(face_recognizer_weights, "")

    COSINE_THRESHOLD = 0.363
    #NORML2_THRESHOLD = 1.128

    # ÁâπÂæ¥„ÇíË™≠„ÅøËæº„ÅøÁâπÂæ¥ÈáèËæûÊõ∏„Çí„Å§„Åè„Çã
    dictionary = []
    files = Path("face_dataset").glob("*.npy")
    for file in files:
        feature = np.load(file)
        user_id = Path(file).stem
        dictionary.append((user_id, feature))

    # ÁâπÂæ¥„ÇíËæûÊõ∏„Å®ÊØîËºÉ„Åó„Å¶„Éû„ÉÉ„ÉÅ„Åó„Åü„É¶„Éº„Ç∂„Éº„Å®„Çπ„Ç≥„Ç¢„ÇíËøî„ÅôÈñ¢Êï∞
    def match(recognizer, feature1, data_directory):
        for element in data_directory:
            user_id, feature2 = element
            score = recognizer.match(feature1, feature2, cv2.FaceRecognizerSF_FR_COSINE)
            if score > COSINE_THRESHOLD:
                return True, (user_id, score)
        return False, ("", 0.0)
    
    recognized_ids =[]
    
    # „Ç´„É°„É©„ÅÆ„Éá„Éï„Ç©„É´„Éà„ÅÆ„Éë„É≥/„ÉÅ„É´„Éà (Â∫¶Âçò‰Ωç)„ÄÇ„Ç≥„Éº„Éâ„ÇíÈñãÂßã„Åô„Çã„Å®„Åç„Å´„ÄÅ„Åä„Åä„Çà„Åù„ÅÆÈ°î„ÅÆ‰ΩçÁΩÆ„ÇíÊåá„Åô„Çà„ÅÜ„Å´Ë®≠ÂÆö„Åó„Åæ„Åó„Åü
    # „Ç´„É°„É©„ÅÆÁØÑÂõ≤„ÅØ 0 ÔΩû 180 „Åß„Åô„ÄÇ‰ª•‰∏ã„ÅÆÂÄ§„ÇíÂ§âÊõ¥„Åó„Å¶„ÄÅ„Éë„É≥„Å®„ÉÅ„É´„Éà„ÅÆÈñãÂßãÁÇπ„ÇíÊ±∫ÂÆö„Åó„Åæ„Åô„ÄÇ
    cam_pan = 90
    cam_tilt = 60
    # „Ç´„É°„É©„ÇíÈñãÂßã‰ΩçÁΩÆ„Å´Âêë„Åë„Åæ„Åô (pan() Èñ¢Êï∞„Å®tilt() Èñ¢Êï∞„ÅåÊúüÂæÖ„Åô„Çã„Éá„Éº„Çø„ÅØ -90 Â∫¶„Åã„Çâ 90 Â∫¶„Åæ„Åß„ÅÆ‰ªªÊÑè„ÅÆÊï∞ÂÄ§„Åß„Åô)
    # pan_tilt(cam_pan-90,cam_tilt-90)

    cam = Camera()  # „Ç´„É°„É©„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Çí‰ΩúÊàê
    # neopixels_all(50, 50, 50)

    # time_start = time.perf_counter()
    # time_end = 0

    # ÂêÑÈ°î„ÅÆID„Åî„Å®„ÅÆÂâç„ÅÆ„Éï„É¨„Éº„É†„ÅÆ„Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„ÅÆÈù¢Á©ç
    previous_areas = {}
    i = 0
    frame_count = 0
    frame_skip = 5
    display_message = ""
    before_most_common_id = ""
    screen_width, screen_height = get_screen_resolution()
    frame_width = int(screen_width * 0.7)
    frame_height = int(screen_height * 0.7)
    with open("data/user_data.json", "r", encoding="utf-8") as f:
        user_data = json.load(f)
    start_time = time.time()

    while True:
        frame = cam.get_frame()  # „Ç´„É°„É©„Åã„Çâ„Éï„É¨„Éº„É†„ÇíÂèñÂæó
        frame = cv2.flip(frame, -1)  # „Ç´„É°„É©ÁîªÂÉè„ÅÆ‰∏ä‰∏ã„ÇíÂÖ•„ÇåÊõø„Åà„Çã

        if frame_count % frame_skip != 0:
            frame_count += 1
            continue

        frame_count += 1

        # ÂÖ•Âäõ„Çµ„Ç§„Ç∫„ÇíÊåáÂÆö„Åô„Çã
        height, width, _ = frame.shape
        face_detector.setInputSize((width, height))

        # È°î„ÇíÊ§úÂá∫„Åô„Çã
        _, faces = face_detector.detect(frame)
        faces = faces if faces is not None else []

        # Ê§úÂá∫„Åó„ÅüÈ°î„ÅÆ„Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ„Å®„É©„É≥„Éâ„Éû„Éº„ÇØ„ÇíÊèèÁîª„Åô„Çã
        frame_output = frame.copy()

        for face in faces:
            # È°î„ÇíÂàá„ÇäÊäú„ÅçÁâπÂæ¥„ÇíÊäΩÂá∫„Åô„Çã
            aligned_face = face_recognizer.alignCrop(frame, face)
            feature = face_recognizer.feature(aligned_face)

            # ËæûÊõ∏„Å®„Éû„ÉÉ„ÉÅ„É≥„Ç∞„Åô„Çã
            result, user = match(face_recognizer, feature, dictionary)

            # „Éû„ÉÉ„ÉÅ„É≥„Ç∞„Åó„Åü„Çâ„Éú„ÉÉ„ÇØ„Çπ„Å®„ÉÜ„Ç≠„Çπ„Éà„ÅÆËâ≤„ÇíÂ§â„Åà„Çã
            if result is True:
                color = (0,255,0)
                # neopixels_all(0, 50, 0)
            else:
                color = (255,255,255)
                # neopixels_all(50, 50, 50)

            # „Éê„Ç¶„É≥„Éá„Ç£„É≥„Ç∞„Éú„ÉÉ„ÇØ„Çπ
            x, y, w, h = list(map(int, face[:4]))
            thickness = 2
            cv2.rectangle(frame_output, (x, y), (x + w, y + h), color, thickness, cv2.LINE_AA)

            # Âá¶ÁêÜÈÄüÂ∫¶Âêë‰∏ä„ÅÆ„Åü„ÇÅ„ÄÅ„É©„É≥„Éâ„Éû„Éº„ÇØ„ÅÆÊèèÁîª„Çí„Ç≥„É°„É≥„Éà„Ç¢„Ç¶„Éà
            # # „É©„É≥„Éâ„Éû„Éº„ÇØÔºàÂè≥ÁõÆ„ÄÅÂ∑¶ÁõÆ„ÄÅÈºª„ÄÅÂè≥Âè£Ëßí„ÄÅÂ∑¶Âè£ËßíÔºâ
            # landmarks = list(map(int, face[4:len(face)-1]))
            # landmarks = np.array_split(landmarks, len(landmarks) / 2)
            # for landmark in landmarks:
            #     radius = 3
            #     thickness = -1
            #     cv2.circle(frame_output, landmark, radius, color, thickness, cv2.LINE_AA)
            
            # Ë™çË≠ò„ÅÆÁµêÊûú„ÇíÊèèÁîª„Åô„Çã
            id, score = user if result else ("unknown", 0.0)
            text = "{0} ({1:.2f})".format(id, score)
            position = (x, y - 10)
            font = cv2.FONT_HERSHEY_SIMPLEX
            scale = 0.6
            thickness = 1
            cv2.putText(frame_output, text, position, font, scale, color, thickness, cv2.LINE_AA)

            # „Éû„ÉÉ„ÉÅ„É≥„Ç∞„Åó„Åü„ÇâID„Çí‰∏ÄÂ∫¶„Å†„ÅëËøΩÂä†„Åô„Çã
            if result:
                recognized_ids.append(id)
                #print(recognized_ids)

            i += 1
            current_area = w * h
            position = (x, y - 30)
            if i % 1 == 0:
                print(current_area)
                color = (255, 255, 255)
                if id in previous_areas:
                    previous_area = previous_areas[id]
                    diff_range = previous_area / 30
                    if current_area > previous_area + diff_range:
                        print(id + "„ÅØËøë„Å•„ÅÑ„Å¶„ÅÑ„Åæ„Åô")
                        text = "approaching"
                        cv2.putText(frame_output, text, position, font, scale, color, thickness, cv2.LINE_AA)
                    elif current_area < previous_area - diff_range:
                        print(id + "„ÅØÈÅ†„Åñ„Åã„Å£„Å¶„ÅÑ„Åæ„Åô")
                        text = "moving away"
                        cv2.putText(frame_output, text, position, font, scale, color, thickness, cv2.LINE_AA)
                    else:
                        print(id + "„ÅØÈùôÊ≠¢„Åó„Å¶„ÅÑ„Åæ„Åô")
                        text = "not moving"
                        cv2.putText(frame_output, text, position, font, scale, color, thickness, cv2.LINE_AA)
                else:
                    print(id)

            previous_areas[id] = current_area

            # È°î„ÅÆ‰∏≠ÂøÉ„ÇíÊçâ„Åà„Çã
            x = x + (w/2)
            y = y + (h/2)

            # ÁîªÂÉè„ÅÆ‰∏≠ÂøÉ„ÇíÂü∫Ê∫ñ„Å®„Åó„Å¶Ë£úÊ≠£
            turn_x  = float(x - (width / 2))
            turn_y  = float(y - (height / 2))

            # „Ç™„Éï„Çª„ÉÉ„Éà„Éª„Éë„Éº„Çª„É≥„ÉÜ„Éº„Ç∏„Å´Â§âÊèõ
            turn_x  /= float(width / 2)
            turn_y  /= float(height / 2)

            # S„Çπ„Ç±„Éº„É´„Ç™„Éï„Çª„ÉÉ„Éà„ÇíÂ∫¶Êï∞„Å´Â§âÊèõ
            #Ôºà‰∏ã„ÅÆ2.5„ÅÆÂÄ§„ÅØPID„ÅÆÊØî‰æã‰øÇÊï∞„ÅÆ„Çà„ÅÜ„Å™ÂÉç„Åç„Çí„Åó„Åæ„ÅôÔºâ
            turn_x   *= 2.5 # VFOV
            turn_y   *= 2.5 # HFOV
            cam_pan  += -turn_x
            cam_tilt += turn_y

            #print(cam_pan-90, cam_tilt-90)

            # „Éë„É≥/„ÉÅ„É´„Éà0ÔΩû180Â∫¶ „Å´Âõ∫ÂÆö
            cam_pan = max(0,min(180,cam_pan))
            cam_tilt = max(0,min(180,cam_tilt))

            # „Çµ„Éº„Éú„ÅÆÊõ¥Êñ∞
            # pan_tilt(int(cam_pan-90),int(cam_tilt-90))
        
        if frame is not None:
            frame_output = cv2.resize(frame_output, (frame_width, frame_height))
            cv2.putText(frame_output, display_message, (0, frame_height-10), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2)
            cv2.imshow("face detection", frame_output)

        elapsed_time = time.time() - start_time
        if elapsed_time >= 5:
            if recognized_ids:
                most_common_id = Counter(recognized_ids).most_common(1)[0][0]
                if most_common_id != before_most_common_id:
                    name = user_data.get(most_common_id, {}).get("name", "unknown")
                    send_message = f"Ââç„Å´„ÅÑ„Çã„ÅÆ„ÅØ{name}„Åï„Çì„Åß„Åô„ÄÇ"
                    display_message = f"Hello, {most_common_id}!"
                    speak_message = f"„Åì„Çì„Å´„Å°„ÅØ„ÄÅ{name}„Åï„ÇìÔºÅ"
                    subprocess.Popen(['./jvoice.sh', speak_message])
                    before_most_common_id = most_common_id
            else:
                display_message = ""
            recognized_ids.clear()
            start_time = time.time()

        # time_end = time.perf_counter() - time_start
        # if time_end > 5:
        #     break

        key = cv2.waitKey(1)
        if key == ord('q'):
            break

    cam.release_camera()  # „Ç´„É°„É©„ÇíËß£Êîæ
    cv2.destroyAllWindows()
    time.sleep(0.5)
    # pan_tilt(0,0)
    # neopixels_off()
    if recognized_ids:
        return [item[0] for item in Counter(recognized_ids).most_common(7)]
    else:
        return []
    
if __name__ == '__main__':
    # print(get_screen_resolution())
    recognized_id = face_recognize()
    print(recognized_id)
